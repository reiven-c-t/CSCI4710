{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\">Map Reduce</h1>\n",
    "<h2  align=\"center\">Lab 2</h2>\n",
    "<h4 align=\"center\">\n",
    "INET4710 Spring 2018<br>\n",
    "Submitted by (your name here)</h4>\n",
    "</font>\n",
    "\n",
    "---------------\n",
    "\n",
    "Lab Objectives\n",
    "-\tUnderstand the map-reduce programming framework\n",
    "-\tLearn how to write a simple map reduce pipeline in Python (single input, single output).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Map Reduce Program:  Word Count\n",
    "\n",
    "Modified from  “A Simple Example in Python” <br> at\n",
    "<https://www.princeton.edu/researchcomputing/computational-hardware/hadoop/mapred-tut/>\n",
    "\n",
    "**1. Create a text file with the text in the poem **\n",
    "[The Road Not Taken](https://www.poetryfoundation.org/poems/44272/the-road-not-taken)\n",
    "\n",
    "**2. In the code section below, write code to read the file and write code for the mapper  function. The code should **\n",
    "-\tCount words\n",
    "-   Run successfully\n",
    "-   Remove punctuation\n",
    "-\tConvert text to lower case\n",
    "\n",
    "\n",
    "This URLs may be helpful:<br>\n",
    "[stdin encoding](http://stackoverflow.com/questions/16549332/python-3-how-to-specify-stdin-encoding)<br>\n",
    "[map reduce example](https://github.com/cielavenir/procon/blob/master/hackerrank/map-reduce-advanced-count-number-of-friends.py)\n",
    "[python dict](https://www.tutorialspoint.com/python/python_dictionary.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 3)\n('about', 1)\n('ages', 2)\n('all', 1)\n('and', 9)\n('another', 1)\n('as', 5)\n('back', 1)\n('be', 2)\n('because', 1)\n('bent', 1)\n('better', 1)\n('black', 1)\n('both', 2)\n('by', 1)\n('claim', 1)\n('come', 1)\n('could', 2)\n('day', 1)\n('difference', 1)\n('diverged', 2)\n('doubted', 1)\n('down', 1)\n('equally', 1)\n('ever', 1)\n('fair', 1)\n('far', 1)\n('first', 1)\n('for', 2)\n('grassy', 1)\n('had', 2)\n('has', 1)\n('having', 1)\n('hence', 1)\n('how', 1)\n('i', 8)\n('if', 1)\n('in', 4)\n('it', 2)\n('i—', 1)\n('just', 1)\n('kept', 1)\n('knowing', 1)\n('lay', 1)\n('leads', 1)\n('leaves', 1)\n('less', 1)\n('long', 1)\n('looked', 1)\n('made', 1)\n('morning', 1)\n('no', 1)\n('not', 1)\n('oh', 1)\n('on', 1)\n('one', 3)\n('other', 1)\n('passing', 1)\n('perhaps', 1)\n('really', 1)\n('roads', 2)\n('same', 1)\n('shall', 1)\n('should', 1)\n('sigh', 1)\n('somewhere', 1)\n('sorry', 1)\n('step', 1)\n('stood', 1)\n('telling', 1)\n('that', 3)\n('the', 8)\n('them', 1)\n('then', 1)\n('there', 1)\n('this', 1)\n('though', 1)\n('to', 2)\n('took', 2)\n('travel', 1)\n('traveled', 1)\n('traveler', 1)\n('trodden', 1)\n('two', 2)\n('undergrowth', 1)\n('wanted', 1)\n('was', 1)\n('way', 2)\n('wear', 1)\n('where', 1)\n('with', 1)\n('wood', 2)\n('worn', 1)\n('yellow', 1)\n('yet', 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "class MapReduce:\n",
    "    def __init__(self):\n",
    "        self.intermediate = dict()\n",
    "        self.result = []\n",
    "\n",
    "    def emitIntermediate(self, key, value):\n",
    "        self.intermediate.setdefault(key, [])\n",
    "        self.intermediate[key].append(value)\n",
    "\n",
    "    def emit(self, value):\n",
    "        self.result.append(value)\n",
    "\n",
    "    def execute(self, data, mapper, reducer):\n",
    "        for record in data:\n",
    "            mapper(record)\n",
    "\n",
    "        for key in sorted(self.intermediate.keys()):\n",
    "            # print(key, self.intermediate[key])\n",
    "            reducer(key, self.intermediate[key])\n",
    "\n",
    "        self.result.sort()\n",
    "        for item in self.result:\n",
    "            print(item)\n",
    "\n",
    "\n",
    "mapReducer = MapReduce()\n",
    "\n",
    "\n",
    "def mapper(line):\n",
    "    \"\"\"\n",
    "    split line to word here and return (word, 1)\n",
    "    :param line: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for c in string.punctuation:\n",
    "        line = line.replace(c, \"\")\n",
    "    line = line.strip().lower()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        mapReducer.emitIntermediate(word, word)  # don't need word as value I think\n",
    "\n",
    "\n",
    "def reducer(key, list_of_values):\n",
    "    \"\"\"\n",
    "    :param key: \n",
    "    :param list_of_values: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    mapReducer.emit((key, len(list_of_values)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inputData = []\n",
    "    file = open(\"/Users/reiven/Documents/Python/CSCI4710/Lab2/TheRoadNotTaken.txt\", \"r\")\n",
    "    for line in file:\n",
    "        inputData.append(line)\n",
    "    mapReducer.execute(inputData, mapper, reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Map Reduce Program:  Natural Language Toolkit\n",
    "\n",
    "Following along with the tutorial:\n",
    "Let us consider a more advanced example, which is based on word counting. A typical text mining or natural language processing task involves following steps:\n",
    " \n",
    "1. Tokenization\n",
    "2. Text normalization\n",
    "3. Stop word removal\n",
    "4. Stemming\n",
    "\n",
    "For more information about NLTK, browse to https://pypi.python.org/pypi/nltk\n",
    "\n",
    "NLTK is already installed as part of the Anaconda installation; however, we need to download the stopwords corpus before running the tutorial NLTK program. Please refer to the lab 2 assignment for instructions about downloading the stopwords corpus.\n",
    "\n",
    "For this section:\n",
    "\n",
    "1. Download the stopwords corpus.\n",
    "2. Modify the NLTK code given in the lab 2 assignment to run in the Map Reduce class for this lab.\n",
    "3. Run the code on \"The Path Not Taken\" poem file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import nltk.tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class MapReduce:\n",
    "    def __init__(self):\n",
    "        self.intermediate = dict()\n",
    "        self.result = []\n",
    "\n",
    "    def emitIntermediate(self, key, value):\n",
    "        self.intermediate.setdefault(key, [])\n",
    "        self.intermediate[key].append(value)\n",
    "\n",
    "    def emit(self, value):\n",
    "        self.result.append(value)\n",
    "\n",
    "    def execute(self, data, mapper, reducer):\n",
    "        for record in data:\n",
    "            mapper(record)\n",
    "\n",
    "        for key in sorted(self.intermediate.keys()):\n",
    "            #            print(key, self.intermediate[key])\n",
    "            reducer(key, self.intermediate[key])\n",
    "\n",
    "        self.result.sort()\n",
    "        for item in self.result:\n",
    "            print(item)\n",
    "\n",
    "\n",
    "mapReducer = MapReduce()\n",
    "\n",
    "\n",
    "def mapper(line):\n",
    "    # init stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    # init stemmer\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "    # remove punctuation\n",
    "    for c in string.punctuation:\n",
    "        line = line.replace(c, \"\")\n",
    "\n",
    "    # remove number\n",
    "    re.sub(\"\\d+\", \"\", line)\n",
    "\n",
    "    line = line.strip().lower()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        # remove stopwords\n",
    "        if word in stops:\n",
    "            word = \"\"\n",
    "        word = stemmer.stem(word)\n",
    "        if word != \"\":\n",
    "            mapReducer.emitIntermediate(word, word)  # don't need word as value I think\n",
    "\n",
    "\n",
    "# add mapper code below\n",
    "\n",
    "def reducer(key, list_of_values):\n",
    "    mapReducer.emit((key, len(list_of_values)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # initialize stop words\n",
    "    # nltk.download()\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    inputData = []\n",
    "    file = file = open(\"/Users/reiven/Documents/Python/CSCI4710/Lab2/TheRoadNotTaken.txt\", \"r\")\n",
    "    for line in file:\n",
    "        inputData.append(line)\n",
    "    mapReducer.execute(inputData, mapper, reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Map Reduce Program:  pyspark Word Count\n",
    "\n",
    "\n",
    "1. Modify the code at\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/wordcount.py\n",
    "2. Run the code on text read from the \"The Path Not Taken\" file\n",
    "\n",
    "The code should:\n",
    "- Count words using pyspark\n",
    "- Remove punctuation\n",
    "- Convert text to lower case\n",
    "- Run successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from operator import add\n",
    "import string\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    for c in string.punctuation:\n",
    "        word = word.replace(c, \"\")\n",
    "    return word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # if len(sys.argv) != 2:\n",
    "    #     print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "    #     exit(-1)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"lab2\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    lines = spark.read.text(\"/Users/reiven/Documents/Python/CSCI4710/Lab2/TheRoadNotTaken.txt\").rdd.map(lambda r: r[0])\n",
    "    # remove punctuation and lowercase\n",
    "    counts = lines.flatMap(lambda x: remove_punctuation(x).split(' ')) \\\n",
    "                  .map(lambda x: x.strip().lower())\\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add)\n",
    "    output = counts.collect()\n",
    "    for (word, count) in output:\n",
    "        print(\"%s: %i\" % (word, count))\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
