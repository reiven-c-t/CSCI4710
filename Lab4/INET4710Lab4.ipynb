{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#483D8B\">\n",
    "<h1  align=\"center\">Matrix and Graph Data Types</h1>\n",
    "<h2  align=\"center\">Lab 4</h2>\n",
    "<h4 align=\"center\">\n",
    "INET4710 Spring 2018<br>\n",
    "Submitted by (your name here)</h4>\n",
    "</font>\n",
    "\n",
    "---------------\n",
    "\n",
    "### Lab Objectives\n",
    "\n",
    "* Write scalable operations using the following data types:\n",
    "    - Vector\n",
    "    - Matrix\n",
    "        - Distributed Matrix\n",
    "    - Graph\n",
    "\n",
    "\n",
    "* Code matrix operations, graph operations, and data type conversions in:\n",
    "    - scipy\n",
    "    - pyspark\n",
    "\n",
    "\n",
    "* Learn a few useful basic machine learning techniques\n",
    "    - Matrix\n",
    "        - SVD (Singular Value Decomposition)\n",
    "        - SGD (Stochastic Gradient Descent)\n",
    "    - Graph\n",
    "        - Find Shortest Distance\n",
    "\n",
    "Instructions: <br>\n",
    "** For this lab, please submit the Jupyter notebook and include answers to the 10 questions. **\n",
    "\n",
    "Notes: <br>\n",
    "- ** scipy.linalg vs numpy.linalg ** <br>\n",
    "from https://docs.scipy.org/doc/scipy/reference/tutorial/linalg <br>\n",
    "scipy.linalg contains all the functions in numpy.linalg. plus some other more advanced ones not contained in numpy.linalg. (For this reason, please use scipy.linalg) \n",
    "\n",
    "\n",
    "- ** matrix subclass of ndarray ** <br>\n",
    "from http://currents.soest.hawaii.edu/ocn_data_analysis/numpy_tutorial.html <br>\n",
    "Numpy includes a matrix subclass of the ndarray base class. Please ignore it. It is not commonly used, it provides only small advantages and only under highly restricted circumstances, and elsewhere it can cause hard-to-diagnose problems. (Please use 2D numpy.ndarray objects instead) \n",
    "\n",
    "\n",
    "- ** scipy documentation ** <br>\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_SciPy_Cheat_Sheet_Linear_Algebra.pdf <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "###  scipy matrix\n",
    "\n",
    "- sparse \n",
    "- dense \n",
    "\n",
    "from https://scipy.github.io/old-wiki/pages/SciPyPackages/Sparse.html <br>\n",
    "A sparse matrix is a two-dimensional matrix with a large number of zero values. In contrast, a matrix where many or most entries are non-zero is said to be dense. There are no strict rules for what constitutes a sparse matrix, so we'll say that a matrix is sparse if there is some benefit to exploiting its sparsity. Additionally, there are a variety of sparse matrix formats which are designed to exploit different sparsity patterns (the structure of non-zero values in a sparse matrix) and different methods for accessing and manipulating matrix entries.\n",
    "\n",
    "Sparsity Patterns:\n",
    "- Diagonal\n",
    "- Block\n",
    "- Unstructured\n",
    "- Sensitivity of pattern to ordering, and use of reordering for locality (e.g. direct solvers)\n",
    "\n",
    "Each sparse format has certain advantages and disadvantages. For instance, adding new non-zero entries to a lil_matrix is fast, however changing the sparsity pattern of a csr_matrix requires a significant amount of work. On the other hand, operations such as matrix-vector multiplication and matrix-matrix arithmetic are much faster with csr_matrix than lil_matrix. A good strategy is to construct matrices using one format and then convert them to another that is better suited for efficient computation.\n",
    "\n",
    "The scipy.sparse module provides data structures for 2D sparse matrices. There are seven sparse matrix types:\n",
    "\n",
    "- csc_matrix: Compressed Sparse Column format\n",
    "- csr_matrix: Compressed Sparse Row format\n",
    "- bsr_matrix: Block Sparse Row format\n",
    "- lil_matrix: List of Lists format\n",
    "- dok_matrix: Dictionary of Keys format\n",
    "- coo_matrix: COOrdinate format (aka IJV, triplet format)\n",
    "- dia_matrix: DIAgonal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n<1x10 sparse matrix of type '<class 'numpy.float64'>'\n\twith 3 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# sparse spark vector\n",
    "# demonstrate sample code to create scipy sparse vectors\n",
    "import numpy as np\n",
    "import scipy.sparse# sparse spark vector\n",
    "\n",
    "# code adapted from https://stackoverflow.com/questions/2540059/scipy-sparse-arrays\n",
    "n = 10\n",
    "# create a 50% sparse vector of random numbers\n",
    "x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "print( type(x) )\n",
    "\n",
    "# convert numpy vector to csr vector\n",
    "x_csr = scipy.sparse.csr_matrix(x)\n",
    "# convert numpy vector to dok vector\n",
    "x_dok = scipy.sparse.dok_matrix(x.reshape(x_csr.shape))\n",
    "\n",
    "print ( repr(x_csr) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix multiplication: numpy\n",
      "0.0016523836402484449\n",
      "\n",
      "matrix multiplication: dictionary of keys\n",
      "0.36879054901055497\n",
      "\n",
      "matrix multiplication: compressed sparse row multiply sum\n",
      "0.022766459691411\n",
      "\n",
      "matrix multiplication: compressed sparse row transpose\n",
      "0.023723507979411806\n"
     ]
    }
   ],
   "source": [
    "# sparse spark vector\n",
    "# performance comparison of sparse data structures\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# create a 50% sparse vector of random numbers\n",
    "def mm_numpy():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    np.dot(x, x)\n",
    "\n",
    "def mm_dict():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_dok = scipy.sparse.dok_matrix(x.reshape(x_csr.shape))\n",
    "    x_dok * x_dok.T\n",
    "\n",
    "def mm_csrmult():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr.multiply(x_csr).sum()\n",
    "    \n",
    "def mm_csrdot():\n",
    "    n = 1000\n",
    "    x = (np.random.rand(n) * 2).astype(int).astype(float) \n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr * x_csr.T\n",
    "    \n",
    "print (\"matrix multiplication: numpy\")\n",
    "t = timeit.timeit(\"mm_numpy()\", setup='import numpy as np; from __main__ import mm_numpy', number=50)\n",
    "print ( t )\n",
    "\n",
    "print (\"\\nmatrix multiplication: dictionary of keys\")\n",
    "t = timeit.timeit('mm_dict()', setup='import numpy as np; from __main__ import mm_dict', number=50)\n",
    "print ( t )\n",
    "\n",
    "print (\"\\nmatrix multiplication: compressed sparse row multiply sum\")\n",
    "t = timeit.timeit(\"mm_csrmult()\", setup='import numpy as np; from __main__ import mm_csrmult', number=50)\n",
    "print ( t )\n",
    "\n",
    "print (\"\\nmatrix multiplication: compressed sparse row transpose\")\n",
    "t = timeit.timeit(\"mm_csrdot()\", setup='import numpy as np; from __main__ import mm_csrdot', number=50)\n",
    "print ( t )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1. Modify the sparse vector performance code above to create two-dimensional matrices and to loop through a few iterations of different sized matrices. Plot the result (matrix size or iteration number vs. time). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparse spark vector\n",
    "# performance comparison of sparse data structures\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# loop five times\n",
    "perf_numpy = np.zeros((5, 2))\n",
    "perf_dict = np.zeros((5, 2))\n",
    "perf_csrmult = np.zeros((5, 2))\n",
    "perf_csrdot = np.zeros((5, 2))\n",
    "\n",
    "\n",
    "# create a 50% sparse vector of random numbers\n",
    "def mm_numpy(i):\n",
    "    n = 100 * i\n",
    "    x = (np.random.rand(n ** 2) * 2).astype(int).astype(float).reshape((n, n))\n",
    "    np.dot(x, x)\n",
    "\n",
    "\n",
    "def mm_dict(i):\n",
    "    n = 100 * i\n",
    "    x = (np.random.rand(n ** 2) * 2).astype(int).astype(float).reshape((n, n))\n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_dok = scipy.sparse.dok_matrix(x.reshape(x_csr.shape))\n",
    "    x_dok * x_dok.T\n",
    "\n",
    "\n",
    "def mm_csrmult(i):\n",
    "    n = 100 * i\n",
    "    x = (np.random.rand(n ** 2) * 2).astype(int).astype(float).reshape((n, n))\n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr.multiply(x_csr).sum()\n",
    "\n",
    "\n",
    "def mm_csrdot(i):\n",
    "    n = 100 * i\n",
    "    x = (np.random.rand(n ** 2) * 2).astype(int).astype(float).reshape((n, n))\n",
    "    x_csr = scipy.sparse.csr_matrix(x)\n",
    "    x_csr * x_csr.T\n",
    "\n",
    "\n",
    "# matrix multiplication: numpy\n",
    "for i in range(1, 6):\n",
    "    t = timeit.timeit(\"mm_numpy(i)\", setup='import numpy as np; from __main__ import mm_numpy; i = (' + str(i) + ')',\n",
    "                      number=50)\n",
    "    perf_numpy[i - 1][0] = i\n",
    "    perf_numpy[i - 1, 1] = t\n",
    "\n",
    "# matrix multiplication: dictionary of keys\n",
    "for i in range(1, 6):\n",
    "    t = timeit.timeit('mm_dict(i)', setup='import numpy as np; from __main__ import mm_dict; i = (' + str(i) + ')',\n",
    "                      number=50)\n",
    "    perf_dict[i - 1][0] = i\n",
    "    perf_dict[i - 1, 1] = t\n",
    "\n",
    "# matrix multiplication: compressed sparse row multiply sum\n",
    "for i in range(1, 6):\n",
    "    t = timeit.timeit(\"mm_csrmult(i)\",\n",
    "                      setup='import numpy as np; from __main__ import mm_csrmult; i = (' + str(i) + ')', number=50)\n",
    "    perf_csrmult[i - 1][0] = i\n",
    "    perf_csrmult[i - 1, 1] = t\n",
    "\n",
    "# matrix multiplication: compressed sparse row transpose\n",
    "for i in range(1, 6):\n",
    "    t = timeit.timeit(\"mm_csrdot(i)\", setup='import numpy as np; from __main__ import mm_csrdot; i = (' + str(i) + ')',\n",
    "                      number=50)\n",
    "    perf_csrdot[i - 1][0] = i\n",
    "    perf_csrdot[i - 1, 1] = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VNX9//HXJyGAyCIaVBQRFBQL\nmMgiiBpQNouCWhf05wKuX75aK/p1Qdu6UKnaUqWKRXEDFAUBUUAUZVNQCoSluCCFKgrFBUEEkS2Z\n8/vjTELIAklmuTOT9/PBPGbm3jv3fnJJ3jm599xzzTmHiIikrrSgCxARkdhS0IuIpDgFvYhIilPQ\ni4ikOAW9iEiKU9CLiKQ4Bb2ISIpT0IuIpDgFvYhIiqsWdAEAmZmZrkmTJkGXISKSVJYsWfKDc67B\ngZZLiKBv0qQJubm5QZchIpJUzOyr8iynQzciIilOQS8ikuIU9CIiKS4hjtGXZs+ePaxfv56dO3cG\nXUpSqFmzJo0aNSIjIyPoUkQkwSRs0K9fv546derQpEkTzCzochKac45Nmzaxfv16mjZtGnQ5IpJg\nEvbQzc6dOznssMMU8uVgZhx22GH660dESpWwQQ8o5CtA+0pEypLQQS8ikqry8uDOO2HduthvS0Ev\nIhJn+fnQrx8MHQrTp8d+ewp6EZE4ys+H/v3hlVfg4Yfhf/4n9ttU0O/H2rVrOemkk7jhhhto2bIl\nPXr0YMeOHXTp0qVwyIYffviBgnF6Ro0axQUXXEDv3r1p2rQpw4cP57HHHuOUU06hY8eObN68GYAu\nXbowcOBAOnXqRKtWrVi0aBGhUIjmzZuzceNGAEKhEM2aNeOHH34I5GsXkejLz4frroOXX4aHHoJB\ng+Kz3YTtXrmPgQNh+fLorjM7G4YNO+Biq1ev5tVXX+XZZ5/l0ksvZdKkSftd/pNPPmHZsmXs3LmT\nZs2a8eijj7Js2TJuu+02xowZw8CBAwHYvn07H330ER988AHXXnstn3zyCVdeeSVjx45l4MCBzJw5\nk6ysLDIzM6Py5YpIsEIhuOEGGD0aHnwQfv/7+G1bLfoDaNq0KdnZ2QC0bduWtWvX7nf5s846izp1\n6tCgQQPq1atH7969AWjduvU+n7388ssByMnJYevWrWzZsoVrr72WMWPGAPDCCy9wzTXXRP8LEpG4\nC4X8IZoXX4T774f77ovv9pOjRV+Olnes1KhRo/B1eno6O3bsoFq1aoRCIYASfdeLLp+Wllb4Pi0t\njby8vMJ5xbtDmhnHHHMMRxxxBLNnz2bhwoWMHTs26l+PiMRXKAT/+7/w3HPwhz/4oI83tegroUmT\nJixZsgSAiRMnVmod48ePB2D+/PnUq1ePevXqAXD99ddz5ZVXcumll5Kenh6dgkUkEM7Bb38LI0fC\nPffA4MEQxCUvCvpKuOOOOxgxYgSdOnWq9MnS+vXr06lTJwYMGMDzzz9fOL1Pnz78/PPPOmwjkuSc\ng1tugREj4O67YciQYEI+XIwL/NG2bVtX3GeffVZiWqro3LmzW7x4canzFi9e7M4444xKrTeV95lI\nMgmFnLvlFufAuTvu8O9jAch15cjY5DhGX0U88sgjjBgxQsfmRZKYc3D77fDkk3DbbfCXvwTYkg9T\n0Adg7ty5pU4fNGgQg+LVsVZEos45P6zBsGFw663wt78FH/KgY/QiIlHhnD8W/7e/+ROwjz+eGCEP\nCnoRkYg5B/feC3/9K9x0EzzxROKEPCjoRUQi4hz88Y/wyCP+oqgnn0yskAcFvYhIRB54wHedvOEG\n+Mc/IC0BU1UnYyvggQceoHbt2mzdupWcnBy6detW6nLLly9nw4YN9OrVK84Vikg8Pfigvwjquuvg\n6acTM+ShHC16M3vBzL43s0+KTDvUzN4zs9Xh5/rh6WZmT5jZGjNbYWZtYll8UAYPHlxmyIMP+unx\nGGRaRALz0EO+Nd+/v7/yNVFDHsp36GYUcE6xaYOAWc655sCs8HuAXwPNw48bgRHRKTM4Q4YM4cQT\nT6Rbt26sWrUKgP79+xcOfbB48WI6depEVlYWp556Kj/99BP33Xcf48ePJzs7u3CoAxFJHQ8/7I/L\nX321H8MmkUMeynHoxjn3gZk1KTb5fKBL+PVoYC5wd3j6mPAVW/80s0PMrKFz7ptIigxqlOIlS5Yw\nbtw4li1bRl5eHm3atKFt27aF83fv3k3fvn0ZP3487du3Z+vWrdSqVYvBgweTm5vL8OHDo1u0iATu\n0Ud9D5srr4QXXoBkGJKqssfojygIb+fcN2Z2eHj60UDROyCuD08rEfRmdiO+1U/jxo0rWUZszZs3\njwsvvJBatWoBfhyaolatWkXDhg1p3749AHXr1o17jSISP0OH+puFXH45jBqVHCEP0T8ZW1qnIlfa\ngs65kcBIgHbt2pW6TIEARykuMZxwUc65/c4XkdTx+OP+qte+fWHMmOQJeah898rvzKwhQPj5+/D0\n9cAxRZZrBGyofHnBysnJYfLkyezYsYNt27YxderUfea3aNGCDRs2sHjxYgC2bdtGXl4ederUYdu2\nbUGULCIx8Pe/+/FrLrnE3wawWpL1V6xs0E8B+oVf9wPeLDL96nDvm47AT5Eenw9SmzZt6Nu3L9nZ\n2Vx00UWceeaZ+8yvXr0648eP55ZbbiErK4vu3buzc+dOzjrrLD777DOdjBVJAU8+6c8TXnQRjB2b\nfCEPYP686X4WMHsVf+I1E/gOuB94A3gNaAx8DVzinNts/jjGcHwvnV+Aa5xzuQcqol27dq7gZtsF\nVq5cyUknnVTRr6dK0z4Tia5//ANuvhkuuABeew0yMoKuaF9mtsQ51+5Ay5Wn183lZczqWsqyDrj5\nwOWJiCS2Z57xId+nD4wfn3ghXxEJ3vtTRCT+nn0WBgyA886DCROgevWgK4qMgl5EpIjnn4cbb4Re\nvWDixOQPeVDQi4gUGjXKD052zjkwaRLUqBF0RdGhoBcRwfeNv/Za6N4dJk+GmjWDrih6FPQiUuW9\n/LIfnKxrV3jjjdQKeVDQJ4XatWsDsHbtWl555ZWAqxFJLa+8Av36wVlnwZtvwkEHBV1R9Cnooywv\nLy9m61bQi0TX+PFw1VWQkwNTpkB4WKuUo6DfjzFjxnDyySeTlZXFVVddBcCECRNo1aoVWVlZ5OTk\nADBq1CguueQSevfuTY8ePfZZx9q1a2nRogXXX389rVq14oorrmDmzJmcfvrpNG/enEWLFgH+piZD\nhw4t/FyrVq1Yu3btPusaNGgQ8+bNIzs7m8cffzyGX7lI6pswAa64As44A6ZNg4MPDrqi2EmKi3kH\nvjOQ5d9Gd5zi7COzGXZO2aOlffrppwwZMoQPP/yQzMxMNm/eDPibjsyYMYOjjz6aLVu2FC6/YMEC\nVqxYwaGHHlpiXWvWrGHChAmMHDmS9u3b88orrzB//nymTJnCn//8Z954441y1fzII48wdOhQpk2b\nVsGvVkSKmjTJj0B52mnw1lupHfKgFn2ZZs+ezcUXX0xmZiZAYYCffvrp9O/fn2effZb8/PzC5bt3\n715qyAM0bdqU1q1bk5aWRsuWLenatStmRuvWrUu02kUktiZPhssugw4dYPp0CJ8CS2lJ0aLfX8s7\nVsoagvjpp59m4cKFvPXWW2RnZ7M8fEeUg/fTJKhRpDNuWlpa4fu0tLTCY/rVqlUjFAoVLrdz586o\nfB0istebb8Kll0L79vD221CnTtAVxYda9GXo2rUrr732Gps2bQIoPHTzn//8hw4dOjB48GAyMzNZ\nt27d/lZTbk2aNGHp0qUALF26lC+//LLEMhr+WKTypk71wwy3aeNDvirdJ0hBX4aWLVvy+9//ns6d\nO5OVlcXtt98OwJ133knr1q1p1aoVOTk5ZGVlRWV7F110EZs3byY7O5sRI0ZwwgknlFjm5JNPplq1\namRlZelkrEgFTJ8OF1/sbyE6YwbUqxd0RfF1wGGK40HDFEeH9plISe+8A+efD61bw8yZcMghQVcU\nPeUdplgtehFJWe++68eSb9nSv06lkK8IBb2IpKSZM31LvkULeO89KKNTXJWgoBeRlDN7NvTuDSec\n4AP/sMOCrihYCnoRSSlz5/obhjRr5kM+fClMlaagF5GU8f77cO650LQpzJoFDRoEXVFiUNCLSEqY\nN8+H/LHH+kM3hx8edEWJQ0EfZ8UHLyvNqFGj2LBhQ5wqEkl+H37ob/13zDE+5I84IuiKEouCPsqi\nMUyxgl6k/BYsgF//Go46yof8kUcGXVHiUdDvRzSGKQYYMmQIJ554It26dWPVqlWF05cvX07Hjh05\n+eSTufDCC/nxxx+ZOHEiubm5XHHFFWRnZ7Njx474fLEiSWjhQujZ04f7nDnQsGHQFSWmpBjU7J2B\n7/Dt8m+jus4js4/knGHnlDk/WsMUL1myhHHjxrFs2TLy8vJo06YNbdu2BeDqq6/mySefpHPnztx3\n3308+OCDDBs2jOHDhzN06FDatTvgBW8iVdbixdCjhz8WP2eOb9FL6dSiL0O0himeN28eF154IbVq\n1aJu3br06dMHgJ9++oktW7bQuXNnAPr168cHH3wQ6y9LJCXk5vqbeGdm+pA/+uigK0psSdGi31/L\nO1aiOUxxaesRkcpZutSHfP36PuSPOSboihKfWvRliNYwxTk5OUyePJkdO3awbds2pk6dCkC9evWo\nX78+8+bNA+Cll14qbN1rOGKR0i1fDt26+dEn58yBxo2Drig5RNSiN7PbgOsBB3wMXAM0BMYBhwJL\ngaucc7sjrDPuig5TnJ6ezimnnMKoUaO48847Wb16Nc45unbtSlZWVmGrvjRt2rShb9++ZGdnc+yx\nx3LmmWcWzhs9ejQDBgzgl19+4bjjjuPFF18EoH///gwYMICDDjqIBQsWcFAq3pZepIJWrPAhX6eO\nD/kmTYKuKHlUephiMzsamA/8yjm3w8xeA6YDvYDXnXPjzOxp4F/OuRH7W5eGKY4O7TNJVR9/DGef\nDTVr+qtfjzsu6IoSQ7yGKa4GHGRm1YBawDfA2cDE8PzRwAURbkNEqrBPP4WuXaFGDd+SV8hXXKWD\n3jn3X2Ao8DU+4H8ClgBbnHMFVw2tB0o9H25mN5pZrpnlbty4sbJliEgK++wz35LPyPAh36xZ0BUl\np0oHvZnVB84HmgJHAQcDvy5l0VKPDTnnRjrn2jnn2jUoY+ShRLj7VbLQvpJU8/nnPuTT0vwVr82b\nB11R8ork0E034Evn3Ebn3B7gdaATcEj4UA5AI6BS1/LXrFmTTZs2KcDKwTnHpk2bqFmzZtCliETF\nqlVw1ln+9Zw5cOKJwdaT7CLpdfM10NHMagE7gK5ALjAHuBjf86Yf8GZlVt6oUSPWr1+PDuuUT82a\nNWnUqFHQZYhEbPVqH/KhkB9bvkWLoCtKfpUOeufcQjObiO9CmQcsA0YCbwHjzOyh8LTnK7P+jIwM\nmjZtWtnyRCQJrVnjQz4vz7fk1YksOiLqR++cux+4v9jkL4BTI1mviFQ9//mPD/ldu/wx+ZYtg64o\ndSTFEAgiktq+/NKH/I4dPuRbtw66otSioBeRQK1d60N++3Z/+7+TTw66otSjoBeRwHz9tQ/5n37y\nIZ+dHXRFqUmDmolIINatgy5dYMsWmDkT2rQJuqLUpRa9iMTd+vW+Jb95M7z3HoTvxSMxoqAXkbja\nsMFf8bpxI7z7LrRvH3RFqU9BLyJx8803viX/7bcwYwZ06BB0RVWDgl5E4uLbb33Ib9jgQ/6004Ku\nqOpQ0ItIzH33nT9cs349vPMOdOoUdEVVi4JeRGLq++/9ePJffQVvvw1nnBF0RVWPgl5EYmbjRh/y\nX3wB06dDTk7QFVVNCnoRiYkffvD3eF2zBt56y/eZl2DogikRibrNm6F7d/j3v2HqVH98XoKjFr2I\nRNXmzb4lv3IlTJniX0uwFPQiEjU//gg9evgber/5pn8twdOhGxGJiv/+F3r2hI8/hsmT4Zxzgq5I\nCijoRSQizsHo0f5GIZ98ApMmQa9eQVclRSnoRaTSvvkG+vSB/v39zUJWrIDzzgu6KilOQS8iFeYc\njB3rW/EzZ8Ljj/sbeTdrFnRlUhoFvYhUyHffwW9+A1deCS1awPLlMHAgpKcHXZmURUEvIuXiHIwf\n71vxb78Nf/0rzJsHJ54YdGVyIAp6ETmgjRvh0kvhssvg+ONh2TK44w614pOFgl5E9mvSJN+KnzIF\nHn4YPvwQTjop6KqkIhT0IlKqTZvg8svh4ouhcWNYsgQGDYJquswy6SjoRaSEN97wrfhJk+Chh2DB\nAmjVKuiqpLL0u1lECm3eDL/7ne86mZ3t7+l68slBVyWRiqhFb2aHmNlEM/vczFaa2WlmdqiZvWdm\nq8PP9aNVrIjEzrRpvhU/fjw88AAsWqSQTxWRHrr5O/COc64FkAWsBAYBs5xzzYFZ4fcikqC2bPFX\ntvbuDQ0a+IC//37IyAi6MomWSge9mdUFcoDnAZxzu51zW4DzgdHhxUYDF0RapIjExttv+2PvL78M\nf/gD5ObCKacEXZVEWyQt+uOAjcCLZrbMzJ4zs4OBI5xz3wCEnw+PQp0iEkU//QTXX+8HH6tXD/75\nT/jTn6B69aArk1iIJOirAW2AEc65U4DtVOAwjZndaGa5Zpa7cePGCMoQkYp47z0/ANmLL/rukkuX\nQrt2QVclsRRJ0K8H1jvnFobfT8QH/3dm1hAg/Px9aR92zo10zrVzzrVr0KBBBGWISHls2wYDBvib\ngRx8MHz0kb8AqkaNoCuTWKt00DvnvgXWmVnBSBddgc+AKUC/8LR+wJsRVSgiEZs1y7fiR46EO+/0\nrfgOHYKuSuIl0n70twBjzaw68AVwDf6Xx2tmdh3wNXBJhNsQkUr6+We4+274xz+geXOYPx86dQq6\nKom3iILeObccKO3oXtdI1isikXv/fbjmGli7Fm67zV/hWqtW0FVJEDQEgkiK2b4dbr0VunSBtDQf\n+I89ppCvyjQEgkgKmT/ft+LXrIFbbvEnWw8+OOiqJGhq0YukgB074P/+D3JyID8f5syBJ55QyIun\nFr1IkluwwA9h8O9/w003waOPQu3aQVcliUQtepEktXMn3HUXnHGGfz1zJjz1lEJeSlKLXiQJLVrk\nW/ErV8KNN/r7t9atG3RVkqjUohdJIrt2wb33wmmn+StdZ8yAZ55RyMv+qUUvkiSWLIF+/eDTT+Ha\na32XyXr1gq5KkoFa9CIJbvduuO8+P2TBjz/CW2/B888r5KX81KIXSWDLl/tW/IoVcPXVMGwY1Nc9\n26SC1KIXSUB79sCDD0L79vD99zBlCowerZCXylGLXiTBrFjhe9QsWwZXXOEvfDr00KCrkmSmFr1I\ngsjLgyFD/E1A/vtfeP11f4s/hbxESi16kQTw6ae+FZ+bC337wvDhkJkZdFWSKtSiFwlQXp4fsqBN\nGz+c8IQJMG6cQl6iSy16kYB8/rlvxS9cCBdd5G8OcvjhQVclqUgtepE4y8+Hv/0NsrNh9Wp49VXf\nklfIS6yoRS8SR//+tx8v/qOP4Pzz4emn4cgjg65KUp1a9CJxEAr5i52ysvxAZC+/DJMnK+QlPtSi\nF4mxNWv82DTz5sF55/lByI46KuiqpCpRi14kRkIh300yK8tfBDVqlL/CVSEv8aYWvUgMfPmlb8XP\nnQvnnAPPPguNGgVdlVRVatGLRFEoBCNGQOvWfljh556D6dMV8hIstehFouSrr+C662DWLOje3Yd8\n48ZBVyWiFr1IxJzzh2Zat/YXPz3zjL/zk0JeEoVa9CIRWLcObrjBB/tZZ8ELL0CTJkFXJbIvtehF\nKsE5ePFFaNXKd5t86imYOVMhL4kp4qA3s3QzW2Zm08Lvm5rZQjNbbWbjzax65GWKJI7//tf3h7/2\nWj+Mwccfw003QZqaTZKgovGteSuwssj7R4HHnXPNgR+B66KwDZHAOQcvveRb8XPmwN//7p+POy7o\nykT2L6KgN7NGwLnAc+H3BpwNTAwvMhq4IJJtiCSCb7+FCy7w921t2RL+9S/43e/UipfkEOm36TDg\nLiAUfn8YsMU5lxd+vx44OsJtiATGOXjlFR/u777rR518/31o3jzoykTKr9JBb2bnAd8755YUnVzK\noq6Mz99oZrlmlrtx48bKliESM2vX+nHir7gCTjgBli+H22+H9PSgKxOpmEha9KcDfcxsLTAOf8hm\nGHCImRV022wEbCjtw865kc65ds65dg0aNIigDJHo2rABbr7Zh/v06fCXv8D8+XDiiUFXJlI5lQ56\n59w9zrlGzrkmwGXAbOfcFcAc4OLwYv2ANyOuUiQOfvgB7rgDjj8eRo70vWrWrIE771QrXpJbLE4l\n3Q3cbmZr8Mfsn4/BNkSiZssWuO8+aNoUHn8cLr0UVq3yNwXRGDWSCqJyZaxzbi4wN/z6C+DUaKxX\nJJZ+/hmefBL++lf48Ue45BJ48EE46aSgKxOJLg2BIFXOzp2+tf7ww/D99/7ipz/9yV/8JJKK1AtY\nqozdu/2AY82awW23+UHIPvoIpk5VyEtqU9BLysvPhzFjoEULGDDAjyo5e7Yfm+a004KuTiT2FPSS\nskIhmDDBD1nQrx8ccgi89RZ8+KEfaVKkqlDQS8pxDqZNg7ZtfQ+atDSYOBFyc6FXL7DSLusTSWEK\nekkps2ZBp07Quzds3eoHIVuxwl/hqnFppKrSt76khAUL4OyzoVs3WL/eX/D0+edw5ZW62ElEQS9J\nbdkyOPdc34r/9FM/dPDq1f6uTxkZQVcnkhgU9JKUPvsMLr4Y2rTxrfmHH4YvvvBDB9esGXR1IolF\nF0xJUvnPf+CBB2DsWKhdG+6/3/eJr1cv6MpEEpeCXpLCunXw0EP+5tsZGX7wsbvugszMoCsTSXwK\neklo333nD8uMGOG7TQ4YAPfeCw0bBl2ZSPJQ0EtC2rzZDzb2xBOwaxf07w9//CMce2zQlYkkHwW9\nJJStW2HYMH/Lvm3b4PLL/TF53bpPpPIU9JIQfvkFnnoKHn0UNm2CCy/0Qwa3bh10ZSLJT90rJVC7\ndsHw4f6uTnfdBe3bw+LF8PrrCnmRaFGLXgKRlwejR8PgwfD115CTA6+9BmeeGXRlIqlHLXqJq1AI\nXn0VfvUruP56OPJIePddmDtXIS8SKwp6iQvn4I03ICsL/t//g4MOgjffhH/+E7p314iSIrGkoJeY\ncg5mzIBTT/UnWHfvhnHj/Bg1ffoo4EXiQUEvMfPBB9C5M5xzDvzwA7z4oh94rG9fDRksEk/6cZOo\nW7QIevb0Ib9mje82uWqVv+ipmk7/i8Sdgl6iZsUKOP986NABli6FoUP9IGQ33QTVqwddnUjVpfaV\nRGzVKn/16vjxULcu/OlPcOutUKdO0JWJCCjoJQJr1/p+8KNH+14099zjR5WsXz/oykSkKAW9VNiG\nDTBkCDz7rD+peuutMGgQHH540JWJSGkU9FJuGzf6sWieespf2Xr99fD730OjRkFXJiL7U+mTsWZ2\njJnNMbOVZvapmd0ann6omb1nZqvDz/pDPslt2eKHCD7uOHj8cd89ctUqP0a8Ql4k8UXS6yYP+D/n\n3ElAR+BmM/sVMAiY5ZxrDswKv5ck9PPP8Oc/Q9Om/u5Ov/41fPIJjBrlQ19EkkOlg945941zbmn4\n9TZgJXA0cD4wOrzYaOCCSIuU+Nq5048Jf/zx/tDMGWf4K1lfew1OOino6kSkoqLSj97MmgCnAAuB\nI5xz34D/ZQCUeorOzG40s1wzy924cWM0ypAI7d4NzzwDzZr5G263bg0LFsDUqZCdHXR1IlJZEQe9\nmdUGJgEDnXNby/s559xI51w751y7Bg0aRFqGRCA/H8aMgRYt/D1Zjz0WZs+GmTOhY8egqxORSEUU\n9GaWgQ/5sc6518OTvzOzhuH5DYHvIytRYiUUggkToFUr6NfP93+fPh3mz4ezzgq6OhGJlkh63Rjw\nPLDSOfdYkVlTgH7h1/2ANytfnsSCczBtGrRtC5de6vvCT5wIubn+hKtGlBRJLZG06E8HrgLONrPl\n4Ucv4BGgu5mtBrqH30sCcA5mzYJOnaB3b3/z7Zde8mPUXHSRAl4kVVX6ginn3HygrGjoWtn1SnRt\n2uTDfcYMfyen9et93/eRI/1okhkZQVcoIrGmK2NTTF6ev2vTu+/6cF+82LfkDzkEunWDXr3g8suh\nZs2gKxWReFHQp4Avv9zbYp81C7Zu9cfdO3SA++/3Y8O3a6ex4EWqKv3oJ6Gff4Y5c/a22lev9tMb\nN/bDE/TsCWefrVEkRcRT0CeBUAiWL9/bav/wQ9izB2rVgi5d4Le/9eF+wgk6oSoiJSnoE9S33+5t\nsb/3nh85EiAry1+12rMnnH461KgRbJ0ikvgU9Ali1y5/odKMGf6xYoWf3qAB9Ojhg717dzjyyGDr\nFJHko6APiHN+qN+CYJ87F3bs8N0dzzgDHn7Yh3tWlj+xKiJSWQr6OPrxx337tH/9tZ9+wgn+Jh49\nevhj7rVrB1qmiKQYBX0M5eX5fuwFrfZFi/yJ1Xr1oGtXuPde32pv0iToSkUklSnoo+yrr/aeRJ01\ny9+dKS0N2reHP/zBt9o7dFCfdhGJH8VNhLZv98fXC8J91So/vVEjP35Mz56+9X7ooYGWKSJVmIK+\ngkIh3yOm4Dj7/Pn+hh0HHQSdO/vx3Hv08HdiUp92EUkECvpy+O4735e9oE/7d9/56a1bw+9+51vt\nZ5yh8WNEJDEp6Euxe7e/+rTgJOry5X56Zqbvy17Qp/2oo4KtU0SkPBT0+D7tq1fv26d9+3Z/wvT0\n02HIEB/up5yiPu0iknyqbNBv2eLvi1oQ7l995ac3a+bHae/Rw99Or06dQMsUEYlYlQn6/Hzfp72g\nd8zChX5anTq+V8zdd/tW+3HHBV2piEh0pXTQr1u3N9hnzvRXppr5sdnvuce32jt21F2WRCS1pVTQ\n//ILvP/+3nBfudJPP+oouOCCvX3aMzODrVNEJJ6SOuidg48/3tunfd48PwpkzZqQk+PHj+nZE371\nK/VpF5GqK6mDfvBgeOAB/7plS7j5Zh/sZ57pL2ASEZEkD/rf/MbfPq9HDzj66KCrEREpnQs59vyy\nh90/7y7xyDwpk0OOPSSm208cpwk8AAAI4UlEQVTqoG/d2j9ERKIllBcqNZAjeezZvqfM7Z074lza\nDWgX068pqYNeRKou5xz5u/NLBuu2yEI5f1d+uWtIr55O9drVSzxqHVaL6rWrk1E7o9T5RR+HHh/7\nEQ8V9CISc86Vfehivy3hn/f/mVBeqNw1ZNQqGbo169Wk7tF1yx3K+zwOrk569fQY7rXoUdCLVAGh\nvBB5u/LI35Vf+vPu/LLnFXnO313KtGLL7dleSjhv3w2ufLVampUarAcffjD1j6tf8UCuXZ2MWhmk\npVfd8UtiEvRmdg7wdyAdeM4590gstiOSaJxzhPJCUQ3S0oK1PMsV3a4LlTNlyyEtI41qNaqRXiO9\nxHPBoYy6x9QtXwjXKTmtWs1qmPpDR1XUg97M0oGngO7AemCxmU1xzn0W7W1J8nPO4UKO0J4Qoby9\nj/w9+Xvfl3PePtP3N6+82wq/rmhLuLwt1/JIr55eIkiLh2v1g6uTfui+09Oqlx3G+zwXW//+Arxg\neUtTCCebWLToTwXWOOe+ADCzccD5QEIFvXMOnO/2VBA2LlQ1plU2LPe3XCTrC4qlG2nV0rBq/rno\n66LP6dXTSauR5p9rppFeL52MjAzSqqeRViPNPxc8ir23DPPP1Us+7/O6mmE1DMvw0y3DoHr4e5Xw\n/2OR54LvYedCe58L5odfh0J7yGc3u0osQxmfC78POdgB7hdXbJnQ3uX22RalzPPvKT6v+PYI+e/L\nousrtp69X+u+6y76mcJl9ln3vvXtsx1K1rPPvi7l885vpNT/j6LP+9RSYn2uxP/pDe3+h+6nXhbT\n7/VYBP3RwLoi79cDHWKwHf7+m9/y7Tu1MOdbGOYMnGGEnx3F3vvl0lzVPVZ3IPlp+YTSQuSn++fi\nj7KmlzqvWohQ9f1/rkLri3Be0enOHC4tik3v0uSFHztiuxnZy1z4QenPUPY8K7qOGC9TtI7Nu4+H\nJAz60v6uK/ETZWY3AjcCNG7cuFIbOqxFJl98uQqf467IHizyXNr/LkWfC/a+/y1r+3zWr9cKli0y\nvextAIQK12/+dwxWdFvhdVlh3ey73cL5rsg69y7nKFjf3s8V1mmucB7hJSkyD8DSHC4thKU5SAth\nBpYeCq/PCstIB6oVvKfgS7TCZ9tnCqW8N8L/inzGyljW75C9uzINLK3IJ9j3s/ssW7CdvfP3qS4U\nbgTkl/Z17F1X4bRSvmbDMLPCdRfU5aeVfO93e7Flyli25HqKbKvoMqQVTvN1ppX+2dK2acXWW2Jb\nJZ8pXE/RbRV73s/nzQyztCLr2Tut+HKUmA+Wllb6fMLLFLxOS6NwjJOiz5WZFsR6TjiBWLOCPyWi\ntkKz04AHnHM9w+/vAXDOPVzWZ9q1a+dyc3OjWoeISKozsyXOuQNebRWLYxiLgeZm1tTMqgOXAVNi\nsB0RESmHqB+6cc7lmdlvgRn4v/5fcM59Gu3tiIhI+cSkH71zbjowPRbrFhGRilH3ExGRFKegFxFJ\ncQp6EZEUp6AXEUlxCnoRkRQX9QumKlWE2Ubgq0p+PBP4IYrlRIvqqhjVVXGJWpvqqphI6jrWOdfg\nQAslRNBHwsxyy3NlWLypropRXRWXqLWproqJR106dCMikuIU9CIiKS4Vgn5k0AWUQXVVjOqquESt\nTXVVTMzrSvpj9CIisn+p0KIXEZH9SIqgN7MXzOx7M/ukjPlmZk+Y2RozW2FmbRKkri5m9pOZLQ8/\n7otTXceY2RwzW2lmn5rZraUsE/d9Vs664r7PzKymmS0ys3+F63qwlGVqmNn48P5aaGZNEqSu/ma2\nscj+uj7WdRXZdrqZLTOzaaXMi/v+KmddQe6vtWb2cXi7JW7AEdOfyb33ckzcB5ADtAE+KWN+L+Bt\n/E2BOgILE6SuLsC0APZXQ6BN+HUd4N/Ar4LeZ+WsK+77LLwPaodfZwALgY7FlrkJeDr8+jJgfILU\n1R8YHu/vsfC2bwdeKe3/K4j9Vc66gtxfa4HM/cyP2c9kUrTonXMfAJv3s8j5wBjn/RM4xMwaJkBd\ngXDOfeOcWxp+vQ1Yib+Xb1Fx32flrCvuwvvg5/DbjPCj+Mmr84HR4dcTga5mBfeEC7SuQJhZI+Bc\n4LkyFon7/ipnXYksZj+TSRH05VDaDckDD5Cw08J/er9tZi3jvfHwn8yn4FuDRQW6z/ZTFwSwz8J/\n7i8Hvgfec86Vub+cc3nAT8BhCVAXwEXhP/Unmtkxsa4pbBhwFxAqY34g+6scdUEw+wv8L+l3zWyJ\n+XtmFxezn8lUCfpy3ZA8AEvxlyhnAU8Cb8Rz42ZWG5gEDHTObS0+u5SPxGWfHaCuQPaZcy7fOZcN\nNAJONbNWxRYJZH+Vo66pQBPn3MnATPa2omPGzM4DvnfOLdnfYqVMi+n+Kmddcd9fRZzunGsD/Bq4\n2cxyis2P2T5LlaBfDxT9zdwI2BBQLYWcc1sL/vR2/q5bGWaWGY9tm1kGPkzHOudeL2WRQPbZgeoK\ncp+Ft7kFmAucU2xW4f4ys2pAPeJ42K6supxzm5xzu8JvnwXaxqGc04E+ZrYWGAecbWYvF1smiP11\nwLoC2l8F294Qfv4emAycWmyRmP1MpkrQTwGuDp+17gj85Jz7JuiizOzIguOSZnYqfn9visN2DXge\nWOmce6yMxeK+z8pTVxD7zMwamNkh4dcHAd2Az4stNgXoF359MTDbhc+gBVlXsWO4ffDnPWLKOXeP\nc66Rc64J/kTrbOfclcUWi/v+Kk9dQeyv8HYPNrM6Ba+BHkDx3nox+5mMyT1jo83MXsX3xsg0s/XA\n/fgTUzjnnsbfn7YXsAb4BbgmQeq6GPhfM8sDdgCXxfqbPex04Crg4/DxXYB7gcZFagtin5WnriD2\nWUNgtJml43+xvOacm2Zmg4Fc59wU/C+ol8xsDb5lelmMaypvXb8zsz5AXriu/nGoq1QJsL/KU1dQ\n++sIYHK4DVMNeMU5946ZDYDY/0zqylgRkRSXKoduRESkDAp6EZEUp6AXEUlxCnoRkRSnoBcRSXEK\nehGRFKegFxFJcQp6EZEU9/8BtxQEWS4UrLUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112400278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plt.plot(perf[:,0],perf[:,1])\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(perf_numpy[:,0], perf_numpy[:,1], color='red', label='numpy')\n",
    "plt.plot( perf_dict[:,0], perf_dict[:,1],  color='blue', label='dict')\n",
    "plt.plot( perf_csrmult[:,0], perf_csrmult[:,1],  color='green', label='csr mult')\n",
    "plt.plot( perf_csrdot[:,0],  perf_csrdot[:,1],   color='purple', label='csr dot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 2. Convert matrix A to CSR format and solve (A A^T) x = b for x.\n",
    "Output the first 10 elements of x. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  15158.53771209  -24853.65199975  -16278.15346026 -294119.00377333\n  -19696.02144414  -16018.57082919  -20839.67298855   -8617.9346644\n  -50139.75641322]\n"
     ]
    }
   ],
   "source": [
    "# from https://scipy.github.io/old-wiki/pages/Additional_Documentation/New_SciPy_Tutorial.html\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg.dsolve as linsolve\n",
    "from numpy import random, linalg\n",
    "\n",
    "# Construct a 1000x1000 lil_matrix\n",
    "A = sparse.lil_matrix((1000, 1000))\n",
    "A[0, :100] = random.rand(100)\n",
    "A[1, 100:200] = A[0, :100]\n",
    "A.setdiag(random.rand(1000))\n",
    "\n",
    "random.seed(1)\n",
    "b = random.rand(1000)\n",
    "\n",
    "A = A.tocsr()  # TODO: it already fulfill question before I do anything...?\n",
    "x = linsolve.spsolve(A * A.T, b)\n",
    "print(x[0:9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 3. matrix conversion of numpy to/from scipy. \n",
    "Write code in the cell below to do what the comments describe. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A - coo matrix format\n<class 'scipy.sparse.coo.coo_matrix'>\n  (0, 0)\t1\n  (0, 2)\t2\n  (1, 1)\t3\n  (3, 3)\t4\n  (1, 1)\t5\n  (0, 0)\t6\n  (0, 0)\t7\n\nA - csr matrix format\n<class 'scipy.sparse.csr.csr_matrix'>\n  (0, 0)\t14\n  (0, 2)\t2\n  (1, 1)\t8\n  (3, 3)\t4\n\nA - dense matrix format\n<class 'numpy.matrixlib.defmatrix.matrix'>\n[[14  0  2  0]\n [ 0  8  0  0]\n [ 0  0  0  0]\n [ 0  0  0  4]]\n\nA - ndarray format\nfix code to make ndarray of A:  <class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n \n[[14  0  2  0]\n [ 0  8  0  0]\n [ 0  0  0  0]\n [ 0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "# matrix conversion of numpy to/from scipy \n",
    "# from https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.coo_matrix.tocsr.html\n",
    "from numpy import array\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# construct a coo matrix\n",
    "row  = array([0,0,1,3,1,0,0])\n",
    "col  = array([0,2,1,3,1,0,0])\n",
    "data = array([1,2,3,4,5,6,7])\n",
    "A = coo_matrix( (data,(row,col)), shape=(4,4))  # TODO: what should I need to do???\n",
    "\n",
    "# output python type and contents of A\n",
    "print (\"A - coo matrix format\")\n",
    "print (type(A))\n",
    "print (A)\n",
    "\n",
    "# question 3 - write code to answer the following:\n",
    "\n",
    "# 1. convert A from coo to csr\n",
    "A = A.tocsr() # TODO: it already finished?\n",
    "\n",
    "# output python type and contents of A in sorted order\n",
    "# note: CSR column indices are not necessarily sorted\n",
    "print (\"\\nA - csr matrix format\")\n",
    "print (type(A))\n",
    "print (A.sorted_indices())\n",
    "\n",
    "# 2. convert A from csr to dense\n",
    "A = A.todense()\n",
    "\n",
    "# output python type and contents of A\n",
    "print (\"\\nA - dense matrix format\")\n",
    "print (type(A))\n",
    "print (A)\n",
    "       \n",
    "# 3. convert dense matrix to ndarray\n",
    "# see https://stackoverflow.com/questions/5183533/how-to-make-list-from-numpy-matrix-in-python\n",
    "print (\"\\nA - ndarray format\")\n",
    "# A = A.ravel().tolist\n",
    "A = array(A)\n",
    "print(\"fix code to make ndarray of A: \", type(A))\n",
    "\n",
    "# output python type and contents of A\n",
    "print (type(A))\n",
    "print (\" \")\n",
    "print (A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "###  pyspark local vector and local matrix\n",
    "\n",
    "- sparse vector\n",
    "- dense vector\n",
    "- sparse matrix \n",
    "- dense matrix (use with caution - watch big data sizes and performance)\n",
    "\n",
    "<br>\n",
    "** Local Vector: ** <br>\n",
    "Generally use the following local vector data types in pyspark 2:\n",
    "- Dense vectors (local):\n",
    "    - numpy ndarray, scipy 1D matrix, python list\n",
    "    - pyspark ml DenseVector\n",
    "- Sparse vectors (local):\n",
    "    - scipy 1D sparse matrix\n",
    "    - pyspark ml SparseVector\n",
    "\n",
    "\n",
    "Both dense vector and sparse vector are homogeneous and can only have numeric data\n",
    "\n",
    "pyspark ml machine learning models also use distributed vectors (and matrices). The distributed structures are introduced later in this lab. Distributed matrices are accessed through the pyspark DataFrame data type, built on top of RDDs.\n",
    "\n",
    "\n",
    "adapted from https://apache.googlesource.com/spark/+/branch-1.1/docs/mllib-data-types.md <br>\n",
    "A pyspark local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. The ml module supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. \n",
    "\n",
    "For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\n",
    "\n",
    "The base class of local vectors is Vector, and we provide two implementations: DenseVector and SparseVector. We recommend using the factory methods implemented in Vectors to create local vectors.\n",
    "\n",
    "DenseVector behaves similarly to a numpy.ndarray or python list, and generally these structures can be used interchangably in pyspark\n",
    "\n",
    "<br>\n",
    "** Local Matrix ** <br>\n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. pyspark ml module supports dense matrices, in which element values are stored in a single double array in column major.\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "###  pyspark data frame\n",
    "\n",
    "from https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ <br>\n",
    "A DataFrame is a distributed collection of rows under named columns. <br>\n",
    "In simple terms, it is similar to a table in relational database or an Excel sheet with Column headers. <br>\n",
    "It also shares some common characteristics with RDD:\n",
    "\n",
    "- Immutable in nature : \n",
    "    - We can create DataFrame / RDD once but can’t change it.\n",
    "    - And we can transform a DataFrame / RDD  after applying transformations.\n",
    "- Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "- Distributed: RDD and DataFrame both are distributed in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up pyspark environment\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# local[*]  Run Spark locally with as many worker threads \n",
    "#           as logical cores on your machine\n",
    "# getOrCreate() Gets an existing SparkSession or, if there \n",
    "#           is no existing one, creates a new one based on the \n",
    "#           options set in this builder\n",
    "\n",
    "spark = SparkSession \\\n",
    "     .builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "     .appName(\"lab4\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scipy sparse csr matrix -> pandas sparse data frame\n",
    "# demonstrate example code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "np.random.seed(1)  # reproduce same result\n",
    "arr = np.random.random(size=(1000, 5))\n",
    "arr[arr < .9] = 0\n",
    "sp_arr = csr_matrix(arr)\n",
    "sdf = pd.SparseDataFrame(sp_arr)\n",
    "sdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[0: double, 1: double, 2: double, 3: double, 4: double]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+---+---+---+\n|  0|                 1|  2|  3|  4|\n+---+------------------+---+---+---+\n|NaN|               NaN|NaN|NaN|NaN|\n|NaN|               NaN|NaN|NaN|NaN|\n|NaN|               NaN|NaN|NaN|NaN|\n|NaN|               NaN|NaN|NaN|NaN|\n|NaN|0.9682615757193975|NaN|NaN|NaN|\n+---+------------------+---+---+---+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# pandas sparse data frame -> pyspark data frame\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)  # instance of SQLContext execution class?\n",
    "spark_df = sqlContext.createDataFrame(sdf)\n",
    "\n",
    "# output data types in pyspark data frame\n",
    "print( spark_df )\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 4. Starting with the ndarray below: **\n",
    "1. ** Convert from the numpy ndarray to a pandas data frame **\n",
    "2. ** Label the columns in the pandas data frame \"random_sample1\" and \"random_sample2\" **\n",
    "3. ** Print the pyspark data frame schema **\n",
    "4. ** Output the first 10 lines in the pyspark data frame **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- random_sample1: double (nullable = true)\n |-- random_sample2: double (nullable = true)\n\n+--------------+------------------+\n|random_sample1|    random_sample2|\n+--------------+------------------+\n|           0.0|               0.0|\n|           0.0|0.9822712283503278|\n|           0.0|               0.0|\n|           0.0|               0.0|\n|           0.0|               0.0|\n|           0.0|               0.0|\n|           0.0|               0.0|\n|           0.0|0.9720306709820733|\n|           0.0|               0.0|\n|           0.0|               0.0|\n+--------------+------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# lab exercise: convert numpy array to pyspark data frame \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nparr = np.random.random(size=(1000, 2))\n",
    "nparr[nparr < .9] = 0\n",
    "\n",
    "# step1: convert the numpy ndarray to a pandas data frame\n",
    "pdf = pd.DataFrame(nparr)\n",
    "# step2: label the feature (i.e., assign a name to the column) in the pandas data frame\n",
    "pdf.columns = ['random_sample1','random_sample2']\n",
    "\n",
    "# step3: convert the pandas data frame to a pyspark data frame\n",
    "spark_df = sqlContext.createDataFrame(pdf)\n",
    "\n",
    "# label the feature (i.e., assign a name to the column) in the pyspark data frame\n",
    "spark_df.printSchema()\n",
    "\n",
    "# step4: output the first 10 rows in the pyspark data frame\n",
    "spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# pyspark sparse vector -> numpy dense vector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "sparseVector = Vectors.sparse(10, [1, 3], [3.0, 4.5])\n",
    "denseVector = sparseVector.toArray()\n",
    "print ( type(denseVector))\n",
    "\n",
    "# Why didn't we convert from a pyspark sparse vector -> spark dense vector\n",
    "# https://forums.databricks.com/questions/8895/converting-dataframe-sparse-vector-column-to-dense.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\nDenseMatrix([[ 1.        ,  0.05564149,         nan,  0.40047142],\n             [ 0.05564149,  1.        ,         nan,  0.91359586],\n             [        nan,         nan,  1.        ,         nan],\n             [ 0.40047142,  0.91359586,         nan,  1.        ]])\n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\nDenseMatrix([[ 1.        ,  0.10540926,         nan,  0.4       ],\n             [ 0.10540926,  1.        ,         nan,  0.9486833 ],\n             [        nan,         nan,  1.        ,         nan],\n             [ 0.4       ,  0.9486833 ,         nan,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "# example pyspark data frame contructed from sparse and dense vectors\n",
    "# https://spark.apache.org/docs/2.2.0/ml-statistics.html\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "print(\" \")\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "\n",
    "###  pyspark distributed matrix operations\n",
    "\n",
    "In Spark 2.0 you have to use correct local types:\n",
    "-\tpyspark.ml.linalg when working DataFrame based pyspark.ml API.\n",
    "-\tpyspark.mllib.linalg when working RDD based pyspark.mllib API.\n",
    "These two namespaces are no longer compatible and require explicit conversions\n",
    "\n",
    "<br>\n",
    "An aside: <br>\n",
    "https://home.apache.org/~pwendell/spark-nightly/spark-branch-2.0-docs/latest/ml-guide.html <br>\n",
    "The MLlib RDD-based API is now in maintenance mode.\n",
    "\n",
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csc.csc_matrix'>\n[[1 0 4]\n [0 0 5]\n [2 3 6]]\n<class 'numpy.ndarray'>\n<class 'pyspark.rdd.RDD'>\n[1 0 4]\n[0 0 5]\n[2 3 6]\nFor large RDD data sizes, better to output a sample number of rows from RDD\n[1 0 4]\n[0 0 5]\n"
     ]
    }
   ],
   "source": [
    "#  scipy sparse matrix to pyspark RDD\n",
    "# https://stackoverflow.com/questions/40645498/create-sparse-rdd-from-scipy-sparse-matrix/40648106#40648106\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "# create a sparse matrix\n",
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6]) \n",
    "sv = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print ( type(sv) )\n",
    "nparray = sv.toarray()\n",
    "print (nparray)\n",
    "\n",
    "print ( type(nparray) )\n",
    "\n",
    "#read sv as RDD\n",
    "sv_rdd = sc.parallelize(sv.toarray())  #convert scipy csc matrix to RDD\n",
    "\n",
    "# memo:\n",
    "# read something to spark service(context) is just use sc.xxx\n",
    "\n",
    "\n",
    "print( type(sv_rdd) )\n",
    "for x in sv_rdd.collect():\n",
    "    print (x)\n",
    "print (\"For large RDD data sizes, better to output a sample number of rows from RDD\")\n",
    "for x in sv_rdd.take(2):\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 5. Why was it necessary to execute collect() before printing each array element? ** <br><br>\n",
    "from https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html <br>\n",
    "** At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. **\n",
    "\n",
    "** collect() collesces the data from all the cluster nodes and transfers the data back to the node executing the driver program. **\n",
    "\n",
    "While running spark in distributed RDD and showing output in a certain computer's standard output, we need to collect data from all the PC to get data we want to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n[((0, 0), DenseMatrix(3, 3, [17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0], 0)), ((1, 0), DenseMatrix(3, 3, [47.0, 52.0, 57.0, 64.0, 71.0, 78.0, 81.0, 90.0, 99.0], 0)), ((0, 1), DenseMatrix(3, 3, [47.0, 64.0, 81.0, 52.0, 71.0, 90.0, 57.0, 78.0, 99.0], 0)), ((1, 1), DenseMatrix(3, 3, [149.0, 166.0, 183.0, 166.0, 185.0, 204.0, 183.0, 204.0, 225.0], 0))]\n"
     ]
    }
   ],
   "source": [
    "# pyspark multiplication using RDD BlockMatrix \n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])), \n",
    "                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks.\n",
    "matA = BlockMatrix(blocks, 3, 2)\n",
    "matB = BlockMatrix(blocks, 3, 2).transpose()\n",
    "\n",
    "# colsPerBlock of this matrix must equal the rowsPerBlock of other\n",
    "amultb = matA.multiply(matB)\n",
    "adotb = amultb.blocks.collect()\n",
    "print( type(adotb) )\n",
    "print( adotb )\n",
    "\n",
    "# note: an improvement over this method is given in:\n",
    "# https://labs.yodas.com/large-scale-matrix-multiplication-with-pyspark-or-how-to-match-two-large-datasets-of-company-1be4b1b2871e\n",
    "# which is much more involved than the BlockMatrix matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 6. Write code in the cell below to execute a pyspark BlockMatrix transpose operation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n[((0, 0), DenseMatrix(2, 3, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 1)), ((0, 1), DenseMatrix(2, 3, [7.0, 8.0, 9.0, 10.0, 11.0, 12.0], 1))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "# Create an RDD of sub-matrix blocks.\n",
    "blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])), \n",
    "                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n",
    "\n",
    "# Create a BlockMatrix from an RDD of sub-matrix blocks.\n",
    "matA = BlockMatrix(blocks, 3, 2)\n",
    "\n",
    "atranspose = matA.transpose()\n",
    "aT = atranspose.blocks.collect()\n",
    "print( type(aT) )\n",
    "print( aT )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 7. Write code in the cell below to execute a pyspark SVD operation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singular vector\n[13.0292755356,5.36857873345,2.53304982188,6.32316604921e-08,2.02269345571e-08]\n\nV factor\nDenseMatrix([[-0.31278534,  0.31167136,  0.30366911,  0.8409913 , -0.07446478],\n             [-0.02980145, -0.17133211, -0.02226069,  0.14664984,  0.97352733],\n             [-0.12207248,  0.15256471, -0.95070998,  0.23828799, -0.03452092],\n             [-0.71847899, -0.68096285, -0.0172245 , -0.02094998, -0.13907533],\n             [-0.60841059,  0.62170723,  0.05606596, -0.46260933,  0.16175873]])\n\nU factor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([-0.3883, -0.9198, -0.0564, 0.0, 0.0]), DenseVector([-0.5302, 0.273, -0.8027, 0.0, 0.0]), DenseVector([-0.7538, 0.2818, 0.5937, 0.0, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "# SVD (Singular Value Decomposition)\n",
    "# from https://spark.apache.org/docs/2.2.0/mllib-dimensionality-reduction.html\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top 5 singular values and corresponding singular vectors.\n",
    "svd = mat.computeSVD(5, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "print(\"singular vector\")\n",
    "print(s)\n",
    "print(\"\\nV factor\")\n",
    "print(V)\n",
    "\n",
    "print(\"\\nU factor\")\n",
    "rowsRDD = U.rows\n",
    "Umatrix = rowsRDD.collect()\n",
    "print(Umatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 8. Complete the pyspark sql code below. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department1\nRow(id='123456', name='Computer Science')\nemployee2\nRow(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\nemail\nno-reply@berkeley.edu\ndf1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\nRow(department=Row(id='123456', name='Computer Science'), employees=[Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000), Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)])\nunion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|          department|\n+--------------------+\n|[123456,Computer ...|\n|[789012,Mechanica...|\n+--------------------+\n\norderBy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(department=Row(id='123456', name='Computer Science'), employees=[Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000), Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)])\nRow(department=Row(id='789012', name='Mechanical Engineering'), employees=[Row(firstName='matei', lastName=None, email='no-reply@waterloo.edu', salary=140000), Row(firstName=None, lastName='wendell', email='no-reply@berkeley.edu', salary=160000)])\n+-------------------+\n|          firstName|\n+-------------------+\n|[michael, xiangrui]|\n|      [matei, null]|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# pyspark sql\n",
    "# from https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(\"department1\")\n",
    "print (department1)\n",
    "print(\"employee2\")\n",
    "print (employee2)\n",
    "print(\"email\")\n",
    "print (departmentWithEmployees1.employees[0].email)\n",
    "\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "print(\"df1\")\n",
    "display(df1)\n",
    "\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "print(\"df2\")\n",
    "display(df2)\n",
    "\n",
    "# 1. write code to output the first row of df1\n",
    "print(\"first\")\n",
    "print(df1.first())\n",
    "\n",
    "# 2. write code to calculate the union of df1 and df2\n",
    "print(\"union\")\n",
    "unionDF = df1.unionAll(df2)\n",
    "display(unionDF)\n",
    "\n",
    "# 3. write code to select the department column from df1\n",
    "print(\"department column\")\n",
    "df1.select(\"department\").show()\n",
    "\n",
    "# 4. write code to output df1 sorted by department name (in ascending order)\n",
    "print(\"orderBy\")\n",
    "d = df1.orderBy([\"department\"]).collect()\n",
    "print(*d, sep = \"\\n\")\n",
    "\n",
    "# 5. write code to select employees.firstName\n",
    "df1.select(\"employees.firstName\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "### graph\n",
    "\n",
    "Calculate single source shortest distances in O(V+E) time for DAGs using Topological Sorting.\n",
    "\n",
    "Initialize distances to all vertices as infinite and distance to source as 0, then find a topological sorting of the graph. Topological Sorting of a graph represents a linear ordering of the graph. Once a topological order is established, process each vertex in topological order. For every vertex processed, update distances of its adjacent using distance of current vertex.\n",
    "\n",
    "How is the graph represented in the shortest path program?\n",
    "\n",
    "The graph is represented using an adjacency list. Each node of adjacency list contains vertex number of the vertex to which edge connects. It also contains weight of the edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph - find shortest distance\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self,vertices):\n",
    "        self.V = vertices\n",
    "        self.graph = defaultdict(list)\n",
    "        \n",
    "    def addEdge(self,u,v,w):\n",
    "        self.graph[u].append((v,w))\n",
    " \n",
    "    def topologicalSortUtil(self,v,visited,stack):\n",
    "        visited[v] = True\n",
    " \n",
    "        if v in self.graph.keys():\n",
    "            for node,weight in self.graph[v]:\n",
    "                if visited[node] == False:\n",
    "                    self.topologicalSortUtil(node,visited,stack)\n",
    " \n",
    "        stack.append(v)\n",
    " \n",
    "    def shortestPath(self, s): \n",
    "        visited = [False]*self.V\n",
    "        stack =[]\n",
    "\n",
    "        for i in range(self.V):\n",
    "            if visited[i] == False:\n",
    "                self.topologicalSortUtil(s,visited,stack)\n",
    "        \n",
    "        dist = [float(\"Inf\")] * (self.V)\n",
    "        dist[s] = 0\n",
    " \n",
    "        while stack:\n",
    "            i = stack.pop()\n",
    "            for node,weight in self.graph[i]:\n",
    "                if dist[node] > dist[i] + weight:\n",
    "                    dist[node] = dist[i] + weight\n",
    "        \n",
    "        for i in range(self.V):\n",
    "            if dist[i] == float(\"Inf\"):\n",
    "                print(str(i) + \" : Inf\")\n",
    "            else:\n",
    "                print(str(i) + \" : \" + str(dist[i]))\n",
    "\n",
    "g = Graph(6)\n",
    "g.addEdge(0, 1, 5)\n",
    "g.addEdge(0, 2, 3)\n",
    "g.addEdge(1, 3, 6)\n",
    "g.addEdge(1, 2, 2)\n",
    "g.addEdge(2, 4, 4)\n",
    "g.addEdge(2, 5, 2)\n",
    "g.addEdge(2, 3, 7)\n",
    "g.addEdge(3, 4, -1)\n",
    "g.addEdge(4, 5, -2)\n",
    " \n",
    "# source = 1\n",
    "s = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 9. Write code (using the python class above) to find the shortest path distances from vertex 1 to other nodes. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are shortest distances from source 1 \n",
      "0 : Inf\n",
      "1 : 0\n",
      "2 : 2\n",
      "3 : 6\n",
      "4 : 5\n",
      "5 : 3\n"
     ]
    }
   ],
   "source": [
    "# source = 1\n",
    "s = 1\n",
    " \n",
    "print (\"Following are shortest distances from source %d \" % s)\n",
    "g.shortestPath(s)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "ONLY FOR YOUR INTEREST. NO NEED TO DO ANYTHING FOR THE LAB HERE. ** WARNING: ADVANCED EXAMPLE ** <br>\n",
    "If you are interested in seeing an application of the concepts in this lab, take a look at: <br>\n",
    "https://blogs.msdn.microsoft.com/data_insights_global_practice/2017/02/16/scaling-up-scikit-learns-random-projection-using-apache-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMO: it seems almost already done when I downloaded this file from canvas, thus I just do just a few fix. :-??\n",
    "# What's happened???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
